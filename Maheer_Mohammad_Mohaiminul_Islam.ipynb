{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fzLC63oezyLr"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "ndqo9YVXzy0b"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "_sA_oSOB0S9R"
      },
      "outputs": [],
      "source": [
        "def initialize_parameters(input_size, output_size):\n",
        "    \"\"\"Initialize weight parameters with small random values.\"\"\"\n",
        "    return np.random.randn(output_size, input_size) * 0.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "CJttcvgj0Ucn"
      },
      "outputs": [],
      "source": [
        "# Task 1: Implement the Softmax Function\n",
        "# The softmax function converts a vector of values to a probability distribution.\n",
        "# Each element is transformed using the exponential function, making them positive,\n",
        "# and then normalized so that the sum of the resulting values is 1.\n",
        "# Implement the softmax function that takes a numpy array `x` as input and returns\n",
        "# a numpy array of the same shape, where each element is the softmax of `x`.\n",
        "def softmax(x):\n",
        "    norm_exp_x = np.exp(x - np.max(x))  # Subtract max for numerical stability\n",
        "    return norm_exp_x / np.sum(norm_exp_x, axis=-1, keepdims=True)  # Maintains the original number of dimensions; ensures that each set of scores is independently normalized"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "S7TdryrGz_09"
      },
      "outputs": [],
      "source": [
        "# Task 2: Implement the Scaled Dot-Product Attention Mechanism\n",
        "# The attention function computes a weighted sum of values V, where the weight assigned\n",
        "# to each value is computed by a compatibility function of the query Q with the corresponding key K.\n",
        "# Implement the function `scaled_dot_product_attention(Q, K, V)` that computes the attention\n",
        "# mechanism's output and the attention weights.\n",
        "# Hint: Use the softmax function you implemented earlier for computing the attention weights.\n",
        "def scaled_dot_product_attention(Q, K, V):\n",
        "\n",
        "    # dotProd = np.dot(Q, K.T)  # Dot product of Q and K^T\n",
        "    dotProd = np.matmul(Q, K.swapaxes(-1, -2))\n",
        "\n",
        "    dim_k = K.shape[1]  # Dimension of the keys\n",
        "    scaled_attention_logits = dotProd / np.sqrt(dim_k)  # Scale by sqrt(dim_k)\n",
        "\n",
        "    # Get the attention weights using softmax\n",
        "    attention_weights = softmax(scaled_attention_logits)\n",
        "\n",
        "    # Multiply the attention weights with V\n",
        "    output = np.matmul(attention_weights, V)\n",
        "\n",
        "    return output, attention_weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "k1AZ6emE0GLr"
      },
      "outputs": [],
      "source": [
        "# Task 3: Implement the Transformer Decoder Layer\n",
        "# A transformer decoder layer consists of a self-attention mechanism, cross-attention with\n",
        "# respect to the encoder outputs, and a position-wise feed-forward network.\n",
        "# Using the `initialize_parameters` function for initializing weights, implement the transformer\n",
        "# decoder layer function `transformer_decoder_layer(Q, K, V, memory, params, mask=None)`.\n",
        "# Use the attention mechanism you defined in Task 2 for both self-attention and cross-attention.\n",
        "# Hint: The decoder outputs should pass through a layer normalization step at the end.\n",
        "def transformer_decoder_layer(Q, K, V, memory, params, mask=None):\n",
        "    # Self attention\n",
        "    self_attention_output, _ = scaled_dot_product_attention(\n",
        "        np.dot(Q, params['W_q']),\n",
        "        np.dot(K, params['W_k']),\n",
        "        np.dot(V, params['W_v'])\n",
        "    )\n",
        "\n",
        "    # Cross-attention with memory\n",
        "    cross_attention_output, _ = scaled_dot_product_attention(\n",
        "        self_attention_output,\n",
        "        np.dot(memory, params['W_m_k']),\n",
        "        np.dot(memory, params['W_m_v'])\n",
        "    )\n",
        "\n",
        "    # Feed-forward\n",
        "    feed_forward_1 = np.dot(cross_attention_output, params['W_ff1']) + params['b_ff1']\n",
        "    feed_forward_1 = np.maximum(0, feed_forward_1)  # ReLU activation\n",
        "    feed_forward_2 = np.dot(feed_forward_1, params['W_ff2']) + params['b_ff2']\n",
        "\n",
        "    # Layer normalization\n",
        "    decoder_output = layer_norm(feed_forward_2)\n",
        "    return decoder_output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "IXLYNxZJ0MNw"
      },
      "outputs": [],
      "source": [
        "#Layer_norm is given as:\n",
        "def layer_norm(x):\n",
        "    return (x - x.mean(axis=-1, keepdims=True)) / np.sqrt(x.var(axis=-1, keepdims=True) + 1e-6)\n",
        "# ----------------------\n",
        "# Parameters Initialization (Provided)\n",
        "# ----------------------\n",
        "\n",
        "d_model = 10  # Embedding size\n",
        "d_ff = 20  # Size of the feed-forward network\n",
        "vocab_size = 10  # Assuming a vocab size of 10 for simplicity\n",
        "\n",
        "# Initialize weights (This part is provided to students)\n",
        "params = {\n",
        "    'W_q': initialize_parameters(d_model, d_model),\n",
        "    'W_k': initialize_parameters(d_model, d_model),\n",
        "    'W_v': initialize_parameters(d_model, d_model),\n",
        "    'W_o': initialize_parameters(d_model, d_model),\n",
        "    'W_m_k': initialize_parameters(d_model, d_model),\n",
        "    'W_m_v': initialize_parameters(d_model, d_model),\n",
        "    'W_ff1': initialize_parameters(d_ff, d_model),\n",
        "    'b_ff1': np.zeros(d_ff),\n",
        "    'W_ff2': initialize_parameters(d_model, d_ff),\n",
        "    'b_ff2': np.zeros(d_model),\n",
        "    'd_model': d_model\n",
        "}\n",
        "\n",
        "# Test Check 1: Softmax Function\n",
        "def check_softmax():\n",
        "    print(\"Checking the softmax function...\")\n",
        "    test_input = np.array([1.0, 2.0, 3.0])\n",
        "    output = softmax(test_input)\n",
        "    if np.allclose(output, np.array([0.09003057, 0.24472847, 0.66524096])):\n",
        "        print(\"Softmax function seems to be implemented correctly.\")\n",
        "    else:\n",
        "        print(\"Softmax function may be incorrect. Please check your implementation.\")\n",
        "\n",
        "# Test Check 2: Scaled Dot-Product Attention\n",
        "def check_attention():\n",
        "    print(\"Checking the attention mechanism...\")\n",
        "    Q = np.array([[1, 0, 0], [0, 1, 0]])\n",
        "    K = V = np.array([[1, 2, 3], [4, 5, 6]])\n",
        "    output, _ = scaled_dot_product_attention(Q, K, V)\n",
        "    # print(\"output.shape\", output.shape)\n",
        "    expected_output = np.array([[3.54902366, 4.54902366, 5.54902366], [3.54902366, 4.54902366, 5.54902366]])\n",
        "    if np.allclose(output, expected_output):\n",
        "        print(\"Attention mechanism seems to be implemented correctly.\")\n",
        "    else:\n",
        "        print(\"Attention mechanism may be incorrect. Please check your implementation.\")\n",
        "\n",
        "# Test Check 3: Transformer Decoder Layer Functionality\n",
        "def check_decoder_layer():\n",
        "    print(\"Checking the transformer decoder layer...\")\n",
        "    Q = K = V = memory = np.random.randn(1, 10, d_model)\n",
        "    output = transformer_decoder_layer(Q, K, V, memory, params)\n",
        "    # Instead of just checking the shape, let's ensure there's a non-zero variance\n",
        "    # across the output, indicating that the layer has applied some transformation.\n",
        "\n",
        "    if output.shape == (1, 10, d_model) and np.var(output) != 0:\n",
        "        print(\"Transformer decoder layer output shape is correct and shows variance across outputs.\")\n",
        "    else:\n",
        "        print(\"There might be an issue with the transformer decoder layer. Please check your implementation.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ncZdja516004",
        "outputId": "49f6c107-914c-4e1e-eab8-7a33fd80acc5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Checking the softmax function...\n",
            "Softmax function seems to be implemented correctly.\n"
          ]
        }
      ],
      "source": [
        "# Uncomment to run checks\n",
        "check_softmax()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "75SloMs0Bll7",
        "outputId": "799abf1b-25bb-45d4-e134-77a03ec6edb7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Checking the attention mechanism...\n",
            "Attention mechanism seems to be implemented correctly.\n"
          ]
        }
      ],
      "source": [
        "check_attention()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IGYuR9kXBmAM",
        "outputId": "944c4c07-f6c9-491b-8a9c-dff95ab449a3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Checking the transformer decoder layer...\n",
            "Transformer decoder layer output shape is correct and shows variance across outputs.\n"
          ]
        }
      ],
      "source": [
        "check_decoder_layer()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
